{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplistic implementation of the two-layer neural network.\n",
    "Training method is stochastic (online) gradient descent with momentum.\n",
    "\n",
    "As an example it computes XOR for given input.\n",
    "\n",
    "Some details:\n",
    "- tanh activation for hidden layer\n",
    "- sigmoid activation for output layer\n",
    "- cross-entropy loss\n",
    "\n",
    "Less than 100 lines of active code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n_hidden = 10\n",
    "n_in = 10\n",
    "n_out = 10\n",
    "n_samples = 300\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "\n",
    "    # forward\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "\n",
    "    # backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riansyah/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/riansyah/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.45465070, Time: 0.0253\n",
      "Epoch: 1, Loss: 0.13697961, Time: 0.0350\n",
      "Epoch: 2, Loss: 0.06206941, Time: 0.0316\n",
      "Epoch: 3, Loss: 0.04092746, Time: 0.0313\n",
      "Epoch: 4, Loss: 0.03159958, Time: 0.0339\n",
      "Epoch: 5, Loss: 0.02592744, Time: 0.0331\n",
      "Epoch: 6, Loss: 0.02199575, Time: 0.0308\n",
      "Epoch: 7, Loss: 0.01907812, Time: 0.0308\n",
      "Epoch: 8, Loss: 0.01682099, Time: 0.0302\n",
      "Epoch: 9, Loss: 0.01502363, Time: 0.0303\n",
      "Epoch: 10, Loss: 0.01356039, Time: 0.0327\n",
      "Epoch: 11, Loss: 0.01234775, Time: 0.0342\n",
      "Epoch: 12, Loss: 0.01132776, Time: 0.0339\n",
      "Epoch: 13, Loss: 0.01045887, Time: 0.0335\n",
      "Epoch: 14, Loss: 0.00971052, Time: 0.0330\n",
      "Epoch: 15, Loss: 0.00905971, Time: 0.0330\n",
      "Epoch: 16, Loss: 0.00848887, Time: 0.0335\n",
      "Epoch: 17, Loss: 0.00798436, Time: 0.0317\n",
      "Epoch: 18, Loss: 0.00753542, Time: 0.0307\n",
      "Epoch: 19, Loss: 0.00713347, Time: 0.0325\n",
      "Epoch: 20, Loss: 0.00677160, Time: 0.0339\n",
      "Epoch: 21, Loss: 0.00644415, Time: 0.0337\n",
      "Epoch: 22, Loss: 0.00614650, Time: 0.0324\n",
      "Epoch: 23, Loss: 0.00587477, Time: 0.0331\n",
      "Epoch: 24, Loss: 0.00562576, Time: 0.0326\n",
      "Epoch: 25, Loss: 0.00539674, Time: 0.0326\n",
      "Epoch: 26, Loss: 0.00518541, Time: 0.0326\n",
      "Epoch: 27, Loss: 0.00498981, Time: 0.0364\n",
      "Epoch: 28, Loss: 0.00480824, Time: 0.0356\n",
      "Epoch: 29, Loss: 0.00463926, Time: 0.0317\n",
      "Epoch: 30, Loss: 0.00448161, Time: 0.0321\n",
      "Epoch: 31, Loss: 0.00433419, Time: 0.0342\n",
      "Epoch: 32, Loss: 0.00419603, Time: 0.0335\n",
      "Epoch: 33, Loss: 0.00406629, Time: 0.0315\n",
      "Epoch: 34, Loss: 0.00394423, Time: 0.0308\n",
      "Epoch: 35, Loss: 0.00382919, Time: 0.0311\n",
      "Epoch: 36, Loss: 0.00372058, Time: 0.0306\n",
      "Epoch: 37, Loss: 0.00361787, Time: 0.0345\n",
      "Epoch: 38, Loss: 0.00352061, Time: 0.0220\n",
      "Epoch: 39, Loss: 0.00342836, Time: 0.0316\n",
      "Epoch: 40, Loss: 0.00334076, Time: 0.0337\n",
      "Epoch: 41, Loss: 0.00325746, Time: 0.0336\n",
      "Epoch: 42, Loss: 0.00317815, Time: 0.0305\n",
      "Epoch: 43, Loss: 0.00310255, Time: 0.0327\n",
      "Epoch: 44, Loss: 0.00303042, Time: 0.0304\n",
      "Epoch: 45, Loss: 0.00296151, Time: 0.0306\n",
      "Epoch: 46, Loss: 0.00289562, Time: 0.0238\n",
      "Epoch: 47, Loss: 0.00283255, Time: 0.0223\n",
      "Epoch: 48, Loss: 0.00277213, Time: 0.0198\n",
      "Epoch: 49, Loss: 0.00271419, Time: 0.0330\n",
      "Epoch: 50, Loss: 0.00265858, Time: 0.0328\n",
      "Epoch: 51, Loss: 0.00260517, Time: 0.0329\n",
      "Epoch: 52, Loss: 0.00255383, Time: 0.0326\n",
      "Epoch: 53, Loss: 0.00250444, Time: 0.0319\n",
      "Epoch: 54, Loss: 0.00245689, Time: 0.0319\n",
      "Epoch: 55, Loss: 0.00241108, Time: 0.0329\n",
      "Epoch: 56, Loss: 0.00236692, Time: 0.0324\n",
      "Epoch: 57, Loss: 0.00232432, Time: 0.0280\n",
      "Epoch: 58, Loss: 0.00228320, Time: 0.0332\n",
      "Epoch: 59, Loss: 0.00224348, Time: 0.0242\n",
      "Epoch: 60, Loss: 0.00220510, Time: 0.0316\n",
      "Epoch: 61, Loss: 0.00216798, Time: 0.0334\n",
      "Epoch: 62, Loss: 0.00213207, Time: 0.0306\n",
      "Epoch: 63, Loss: 0.00209730, Time: 0.0306\n",
      "Epoch: 64, Loss: 0.00206363, Time: 0.0320\n",
      "Epoch: 65, Loss: 0.00203101, Time: 0.0352\n",
      "Epoch: 66, Loss: 0.00199938, Time: 0.0315\n",
      "Epoch: 67, Loss: 0.00196870, Time: 0.0343\n",
      "Epoch: 68, Loss: 0.00193892, Time: 0.0303\n",
      "Epoch: 69, Loss: 0.00191002, Time: 0.0307\n",
      "Epoch: 70, Loss: 0.00188195, Time: 0.0305\n",
      "Epoch: 71, Loss: 0.00185467, Time: 0.0301\n",
      "Epoch: 72, Loss: 0.00182816, Time: 0.0304\n",
      "Epoch: 73, Loss: 0.00180238, Time: 0.0301\n",
      "Epoch: 74, Loss: 0.00177730, Time: 0.0303\n",
      "Epoch: 75, Loss: 0.00175290, Time: 0.0305\n",
      "Epoch: 76, Loss: 0.00172914, Time: 0.0302\n",
      "Epoch: 77, Loss: 0.00170600, Time: 0.0306\n",
      "Epoch: 78, Loss: 0.00168346, Time: 0.0291\n",
      "Epoch: 79, Loss: 0.00166149, Time: 0.0348\n",
      "Epoch: 80, Loss: 0.00164008, Time: 0.0330\n",
      "Epoch: 81, Loss: 0.00161920, Time: 0.0328\n",
      "Epoch: 82, Loss: 0.00159883, Time: 0.0319\n",
      "Epoch: 83, Loss: 0.00157896, Time: 0.0309\n",
      "Epoch: 84, Loss: 0.00155957, Time: 0.0319\n",
      "Epoch: 85, Loss: 0.00154063, Time: 0.0309\n",
      "Epoch: 86, Loss: 0.00152214, Time: 0.0290\n",
      "Epoch: 87, Loss: 0.00150407, Time: 0.0238\n",
      "Epoch: 88, Loss: 0.00148642, Time: 0.0229\n",
      "Epoch: 89, Loss: 0.00146917, Time: 0.0230\n",
      "Epoch: 90, Loss: 0.00145230, Time: 0.0231\n",
      "Epoch: 91, Loss: 0.00143581, Time: 0.0258\n",
      "Epoch: 92, Loss: 0.00141968, Time: 0.0222\n",
      "Epoch: 93, Loss: 0.00140390, Time: 0.0212\n",
      "Epoch: 94, Loss: 0.00138846, Time: 0.0293\n",
      "Epoch: 95, Loss: 0.00137334, Time: 0.0329\n",
      "Epoch: 96, Loss: 0.00135854, Time: 0.0330\n",
      "Epoch: 97, Loss: 0.00134405, Time: 0.0326\n",
      "Epoch: 98, Loss: 0.00132986, Time: 0.0247\n",
      "Epoch: 99, Loss: 0.00131596, Time: 0.0215\n"
     ]
    }
   ],
   "source": [
    "# Setup initial parameters\n",
    "# Note that initialization is cruxial for first-order methods!\n",
    "\n",
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "# Generate some data\n",
    "\n",
    "X = np.random.binomial(1, 0.5, (n_samples, n_in))\n",
    "T = X ^ 1\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "\n",
    "    t0 = time.clock()\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad = train(X[i], T[i], *params)\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %.8f, Time: %.4f\" % (epoch, np.mean( err ), time.clock()-t0 ))\n",
    "\n",
    "# Try to predict something\n",
    "\n",
    "x = np.random.binomial(1, 0.5, n_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR prediction:\n",
      "[1 0 1 1 1 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"XOR prediction:\")\n",
    "print(x)\n",
    "print(predict(x, *params))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
